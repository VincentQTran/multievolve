{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In the previous notebook, we saw how to use the ```splitters```, ```featurizers```, ```predictors```, and ```proposers``` modules to train and deploy models to perform property prediction. In this notebook, we will see how to test multiple methods of data splits, featurizations, and models at once. We will initialize several different ```splitters```, ```featurizers```, and ```predictors``` and test them all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from multievolve.splitters import *\n",
    "from multievolve.featurizers import *\n",
    "from multievolve.predictors import *\n",
    "from multievolve.proposers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up\n",
    "\n",
    "First, define the following variables as before:\n",
    "- ```experiment_name```: the name of the experiment\n",
    "\n",
    "- ```protein_name```: the name of the protein\n",
    "\n",
    "- ```wt_file```: the path to the wildtype sequence\n",
    "\n",
    "- ```training_dataset_fname```: the path to the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name = \"example_experiment\"\n",
    "protein_name = \"example_protein\"\n",
    "wt_file = \"../../data/example_protein/apex.fasta\"\n",
    "training_dataset_fname = '../../data/example_protein/example_dataset.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-neural network models\n",
    "\n",
    "We will show an example for non-neural network models first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize our desired splitters and save them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_splitter = RandomProteinSplitter(protein_name, training_dataset_fname, wt_file, csv_has_header=True, use_cache=True, y_scaling=False, val_split=None)\n",
    "round_splitter = RoundProteinSplitter(protein_name, training_dataset_fname, wt_file, csv_has_header=True, use_cache=True, y_scaling=False, val_split=None)\n",
    "\n",
    "random_splitter.split_data(test_size=0.2)\n",
    "round_splitter.split_data(min_test_round=1, max_train_round=0)\n",
    "\n",
    "splitters = [random_splitter, round_splitter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize our desired featurizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onehot = OneHotFeaturizer(protein=protein_name, use_cache=True)\n",
    "georgiev = GeorgievFeaturizer(protein=protein_name, use_cache=True)\n",
    "\n",
    "featurizers = [onehot, georgiev]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize our desired predictors in the form of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictors = [RidgeRegressor, RandomForestRegressor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the following function ```run_model_experiments``` to train and test all the models. The function will return a pandas dataframe with the results, which are also saved in the directory where the training dataset is located in the folder ```model_cache/dataset_name/results```\n",
    "\n",
    "```run_model_experiments``` takes the following arguments:\n",
    "\n",
    "- ```splitters```: A list of splitters to use for the experiment.\n",
    "\n",
    "- ```featurizers```: A list of featurizers to use for the experiment.\n",
    "\n",
    "- ```predictors```: A list of predictors to use for the experiment.\n",
    "\n",
    "- ```experiment_name```: The name of the experiment.\n",
    "\n",
    "- ```use_cache```: Whether to use the cache for the splitters, featurizers, and predictors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = run_model_experiments(splitters, featurizers, predictors, experiment_name,use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network models\n",
    "\n",
    "Now, we will show an example for neural network models. You will need to have a wandb account to run this example.\n",
    "\n",
    "For neural network models, we used wandb to perform hyperparameter sweeps, which include comparing different methods of splitting and featurizing.\n",
    "\n",
    "We will initialize the same splitters and featurizers as before, and train various fully connected neural networks with different architectures and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_splitter = RandomProteinSplitter(protein_name, training_dataset_fname, wt_file, csv_has_header=True, use_cache=True, y_scaling=False, val_split=0.15)\n",
    "round_splitter = RoundProteinSplitter(protein_name, training_dataset_fname, wt_file, csv_has_header=True, use_cache=True, y_scaling=False, val_split=0.15)\n",
    "\n",
    "random_splitter.split_data(test_size=0.2)\n",
    "round_splitter.split_data(min_test_round=1, max_train_round=0)\n",
    "\n",
    "splitters = [random_splitter, round_splitter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onehot = OneHotFeaturizer(protein=protein_name, use_cache=True) # just as an example, we will just use onehot featurizer\n",
    "\n",
    "featurizers = [onehot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = [Fcn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neural network models, we use the function ```run_nn_model_experiments``` to train and test all the models. Unlike the previous function, the results of the model training are saved on the wandb server.\n",
    "\n",
    "```run_nn_model_experiments``` takes the following arguments:\n",
    "\n",
    "- ```splitters```: A list of splitters to use for the experiment.\n",
    "\n",
    "- ```featurizers```: A list of featurizers to use for the experiment.\n",
    "\n",
    "- ```models```: A list of models to use for the experiment.\n",
    "\n",
    "- ```experiment_name```: The name of the experiment.\n",
    "\n",
    "- ```use_cache```: Whether to use the cache for the splitters, featurizers, and predictors.\n",
    "\n",
    "- ```sweep_depth```: The depth of the hyperparameter sweep. Options are 'test', 'standard', and 'custom'. 'test' is to test the training on a single hyperparameter configuration. 'standard' will test a standard set of hyperparameter configurations. 'custom' is a modified version of 'standard' that can be modified to test a subset of hyperparameter configurations.\n",
    "\n",
    "\n",
    "- ```search_method```: The method of hyperparameter search. Options are 'test', 'grid', and 'bayes'. 'test' is to test the training on a single hyperparameter configuration. 'grid' is to test the training on all possible hyperparameter configurations. 'bayes' is to test the training on a bayesian search of hyperparameter configurations.\n",
    "\n",
    "- ```count```: The number of different hyperparameter configurations to test. This is only used if ```search_method``` is 'bayes'.\n",
    "\n",
    "If the user wants to modify the hyperparameter sweeps to test other hyperparameters, then the configuration files for the sweep configs can be found in ```multievolve/predictors/sweep_configs```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_nn_model_experiments(splitters, \n",
    "                         featurizers, \n",
    "                         models, \n",
    "                         experiment_name=experiment_name,\n",
    "                         use_cache=False,\n",
    "                         sweep_depth='test', \n",
    "                         search_method='test',\n",
    "                         count=1\n",
    "                         )"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "multievolve",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "multievolve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
